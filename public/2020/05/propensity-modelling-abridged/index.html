<!DOCTYPE html>
<html lang="en-UK" />
<head>
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

    <title>Propensity Modelling - Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product - Abridged Version &middot; </title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/favicon.ico" />
    <link rel="canonical" href="/2020/05/propensity-modelling-abridged/" />

     <meta name="description" content="In this day and age, a business that leverages data to understand the drivers of customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can " /> 

     
    
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image" content="/img/dan-meyers-MQ8-4HYTgOc-unsplash.jpg"/>
    
 
    <meta name="twitter:title" content="Propensity Modelling - Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product - Abridged Version"/>
    <meta name="twitter:description" content="In this day and age, a business that leverages data to understand the drivers of customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can "/>
    <meta name="twitter:url" content="/2020/05/propensity-modelling-abridged/" />
    <meta name="twitter:site" content="@"/>

    <meta property="og:site_name" content="" />
    <meta property="og:title" content="Propensity Modelling - Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product - Abridged Version &middot; Lifelong Learning" />
    <meta property="og:url" content="/2020/05/propensity-modelling-abridged/" />
    

    <meta property="og:type" content="article" />
    <meta property="og:description" content="In this day and age, a business that leverages data to understand the drivers of customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can " />

    <meta property="article:published_time" content="2020-05-02T00:00:00Z" />
    <meta property="article:tag" content="Propensity Modelling" /><meta property="article:tag" content="Data Exploration" /><meta property="article:tag" content="ML Interpretability" /><meta property="article:tag" content="Model Selection" /><meta property="article:tag" content="Optimisation" /><meta property="article:tag" content="Machine Learning" />

    <meta property="og:image" content="/img/dan-meyers-MQ8-4HYTgOc-unsplash.jpg"/>


    <meta name="generator" content="Hugo 0.58.3" />

    <!-- Stylesheets -->
    <link rel="stylesheet" type="text/css" href="/built/screen.css" /> 
    <link rel="stylesheet" type="text/css" href="/css/casper-two.css" /> 
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" />
     <link rel="stylesheet" href="/css/override.css" /> 

     

</head>


<body class="post-template">
  <div class="site-wrapper"> 

<header class="site-header outer">
  <div class="inner">
    <nav class="site-nav">
      <div class="site-nav-left">

        <ul class="nav" role="menu">
        
        
        
            <li class="" role="menuitem">
              <a href="/">Home</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/categories/articles/">articles</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/categories/multi-article-studies/">projects</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/categories/data-handling">data</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/tags/">Tags</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/page/links/">Readings</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/page/about/">About</a>
            </li>
        
      </ul></div>

      <div class="site-nav-right">
        <div class="social-links">
                    

                    

                    <a class="social-link" href="https://github.com/DiegoUsaiUK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>

                    <a class="social-link" href="https://www.linkedin.com/in/diegousaiuk" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 50 512 512"><path d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683 C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615 c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915 s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z" /></svg></a>

                    <a class="social-link" href="https://medium.com/@diegousaiuk" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 195 195"><path d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"/></svg></a>
        </div>  
            
      </div>

    </nav>  

  </div>
</header>

<main id="site-main" class="site-main outer" role="main">
  <div class="inner">
    
      <article class="post-full post"> 
    <header class="post-full-header">
        <section class="post-full-meta">
            <time class="post-full-meta-date" datetime="2020-05-02">2 May 2020</time>
                <span class="date-divider">/</span> <a href="/tags/propensity-modelling/">#Propensity Modelling</a>&nbsp;<a href="/tags/data-exploration/">#Data Exploration</a>&nbsp;
        </section>
        <h1 class="post-full-title">Propensity Modelling - Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product - Abridged Version</h1>
    </header>
    
    <figure class="post-full-image" style="background-image: url(/img/dan-meyers-MQ8-4HYTgOc-unsplash.jpg)">
    </figure>

    <section class="post-full-content">
        <div class="kg-card-markdown">
        

<p>In this day and age, a business that leverages data to understand the drivers of customers&rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.</p>

<p>One trialled and tested approach to tease this type of insight out of data is <a href="https://en.wikipedia.org/wiki/Predictive_modelling"><strong>Propensity Modelling</strong></a>, which combines information such as a <strong>customers’ demographics</strong> (age, race, religion, gender, family size, ethnicity, income, education level), <strong>psycho-graphic</strong> (social class, lifestyle and personality characteristics), <strong>engagement</strong> (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.), <strong>user experience</strong> (customer service phone and email wait times, number of refunds, average shipping times), and <strong>user behaviour</strong> (purchase value on different time-scales, number of days since most recent purchase, time between offer and conversion, etc.) to estimate the likelihood of a certain customer profile to performing a certain type of behaviour (e.g. the purchase of a product).</p>

<p>Once you understand the probability of a certain customer to interact with a brand, buy a product or a sign up for a service, you can use this information to create scenarios, be it minimising <strong>marketing expenditure</strong>, maximising <strong>acquisition targets</strong>, and optimise <strong>email send frequency</strong> or <strong>depth of discount</strong>.</p>

<h2 id="project-structure">Project Structure</h2>

<p>In this project I&rsquo;m analysing the results of a bank <strong>direct marketing campaign</strong> to sell term a deposit its existing clients in order to identify what type of characteristics make a customer more likely to respond. The marketing campaigns were based on phone calls and more than one contact to the same person was required at times.</p>

<p>First, I am going to carry out an <strong>extensive data exploration</strong> and use the results and insights to prepare the data for analysis.</p>

<p>Then, I&rsquo;m <strong>estimating a number of models</strong> and assess their performance and fit to the data using a <strong>model-agnostic methodology</strong> that enables to <strong>compare traditional &ldquo;glass-box&rdquo; models and &ldquo;black-box&rdquo; models</strong>.</p>

<p>Last, I&rsquo;ll fit <strong>one final model</strong> that combines findings from the exploratory analysis and insight from models&rsquo; selection and use it to <strong>run a revenue optimisation</strong>.</p>

<h2 id="the-data">The data</h2>

<pre><code class="language-r">library(tidyverse)
library(data.table)
library(skimr)
library(correlationfunnel)
library(GGally)
library(ggmosaic)
library(knitr)
library(h2o)
library(DALEX)
library(knitr)
library(tictoc)
</code></pre>

<p>The Data is the <a href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"><strong>Portuguese Bank Marketing</strong></a> set from the <a href="https://archive.ics.uci.edu/ml/datasets">UCI Machine Learning Repository</a> and describes the direct marketing campaigns carried out by a Portuguese banking institution aimed at selling term deposits/certificate of deposits to their customers. The marketing campaigns were based on phone calls to potential buyers from May 2008 to November 2010.</p>

<p>The data I&rsquo;m using <a href="https://github.com/DiegoUsaiUK/Propensity_Modelling/tree/master/01_data/bank-direct-marketing.csv"><strong>bank-direct-marketing.csv</strong></a> is a modified version of <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip">bank-additional-full.csv</a> and contains 41,188 examples with 21 different variables (10 continuous, 10 categorical plus the target variable). In particular, the target <strong>subscribed</strong> is a <strong>binary response variable</strong> indicating whether the client subscribed (‘Yes’ or numeric value 1) to a term deposit or not (‘No’ or numeric value 0), which make this a <a href="https://en.wikipedia.org/wiki/Binary_classification"><em>binary classification problem</em></a>.</p>

<p>The data required some manipulation to get into a usable format, details of which can be found on my webpage: <a href="https://diegousai.io/2020/02/propensity-modelling-1-of-3/"> <strong>Propensity Modelling - Data Preparation and Exploratory Data Analysis</strong></a>. Here I simply load up the pre-cleansed data I am hosting <a href="https://github.com/DiegoUsaiUK/Propensity_Modelling"> <strong>on my GitHub repo for this project</strong></a></p>

<pre><code class="language-r">data_clean &lt;- 
  readRDS(file = &quot;https://raw.githubusercontent.com/DiegoUsaiUK/Propensity_Modelling/master/01_data/data_clean.rds&quot;)
</code></pre>

<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>

<p>Although an integral part of any Data Science project and crucial to the full success of the analysis, <a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">Exploratory Data Analysis (EDA)</a> can be an incredibly labour intensive and time consuming process. Recent years have seen a proliferation of approaches and libraries aimed at speeding up the process and in this project I&rsquo;m going to sample one of the &ldquo;new kids on the block&rdquo; ( the <a href="https://business-science.github.io/correlationfunnel/"><strong>correlationfunnel</strong></a> ) and combine its results with a more traditional EDA.</p>

<h3 id="correlationfunnel"><em>correlationfunnel</em></h3>

<p>With 3 simple steps <code>correlationfunnel</code> can produce a graph that arranges predictors top to bottom in descending order of absolute correlation with the target variable. Features at the top of the funnel are expected to have have stronger predictive power in a model.</p>

<p>This approach offers a quick way to identify a hierarchy of expected predictive power for all variables and gives an early indication of which predictors should feature strongly/weakly in any model.</p>

<pre><code class="language-r">data_clean %&gt;%  
      # turn numeric and categorical features into binary data
  binarize(n_bins = 4, # bin number for converting to discrete features
           thresh_infreq = 0.01 # thresh. for assign categ. features into &quot;Other&quot;
          ) %&gt;%
      # correlate target variable to features 
  correlate(target = subscribed__1) %&gt;% 
      # correlation funnel visualisation
  plot_correlation_funnel()
</code></pre>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/unnamed-chunk-5-1.png" alt="" width="100%" height="100%"/></p>

<p>Zooming in on the top 5 features we can see that certain characteristics have a greater correlation with the target variable (subscribing to the term deposit product) when:</p>

<ul>
<li>The <code>duration</code> of the last phone contact with the client is 319 seconds or longer</li>
<li>The number of <code>days</code> that passed by after the client was last contacted is greater than 6</li>
<li>The outcome of the <code>previous</code> marketing campaign was <code>success</code></li>
<li>The number of employed is 5,099 thousands or higher</li>
<li>The value of the euribor 3 month rate is 1.344 or higher</li>
</ul>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/unnamed-chunk-6-1.png" alt="" width="100%" height="100%"/></p>

<p>Conversely, variables at the bottom of the funnel, such as <strong>day_of_week</strong>, <strong>housing</strong>, and <strong>loan</strong>. show very little variation compared to the target variable (i.e.: they are very close to the zero correlation point to the response). For that reason, I&rsquo;m not expecting these features to impact the response.</p>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/unnamed-chunk-7-1.png" alt="" width="100%" height="100%"/></p>

<h3 id="features-exploration">Features exploration</h3>

<p>Guided by the results of this visual correlation analysis, I will continue to explore the relationship between the target and each of the predictors in the next section. For this I am going to enlist the help of the brilliant <strong>GGally</strong> library to visualise a modified version of the correlation matrix with <code>Ggpairs</code>, and plot <code>mosaic charts</code> with the <strong>ggmosaic</strong> package, a great way to examine the relationship among two or more categorical variables.</p>

<h4 id="target-variable">Target Variable</h4>

<p>First things first, the <strong>target variable</strong>: <code>subscribed</code> shows a <strong>strong class imbalance</strong>, with nearly 89% in the <strong>No category</strong> to 11% in the <strong>Yes category</strong>.</p>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/unnamed-chunk-8-1.png" alt="" width="100%" height="100%"/></p>

<p>I am going to address <strong>class imbalance</strong>  during the modelling phase by enabling  <strong>re-sampling</strong>, in <strong>h2o</strong>. This will rebalance the dataset by &ldquo;shrinking&rdquo; the prevalent class (&ldquo;No&rdquo; or 0) and ensure that the model adequately detects what variables are driving the ‘yes’ and ‘no’ responses.</p>

<h4 id="predictors">Predictors</h4>

<p>Let&rsquo;s continue with <strong>some of the numerical features</strong>:</p>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/some continuous features-1.png" alt="" width="100%" height="100%"/></p>

<p>Although the correlation funnel analysis revealed that <strong>duration</strong> has the strongest expected predictive power, it is unknown before a call (it’s obviously known afterwards) and offers very little actionable insight or predictive value. Therefore, it should be discarded from any realistic predictive model and will not be used in this analysis.</p>

<p><strong>age</strong> &rsquo;s density plots have very similar variance compared to the target variable and are centred around the same area. For these reasons, it should not have a great impact on <strong>subscribed</strong>.</p>

<p>Despite continuous in nature, <strong>pdays</strong> and <strong>previous</strong> are in fact categorical features and are also all strongly right skewed. For these reasons, they will need to be discretised into groups. Both variables are also moderately correlated, suggesting that they may capture the same behaviour.</p>

<p>Next, I visualise the <strong>bank client data</strong> with the <em>mosaic charts</em>:</p>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/bank client data all-1.png" alt="" width="100%" height="100%"/></p>

<p>In line with the <em>correlationfunnel</em> findings, <strong>job</strong>, <strong>education</strong>, <strong>marital</strong> and <strong>default</strong> all show a good level of variation compared to the target variable, indicating that they would impact the response. In contrast, <strong>housing</strong> and <strong>loan</strong> sat at the very bottom of the funnel and are expected to have little influence on the target, given the small variation when split by &ldquo;subscribed&rdquo; response.</p>

<p><strong>default</strong> has only 3 observations in the ‘yes’ level, which will be rolled into the least frequent level as they&rsquo;re not enough to make a proper inference. Level ‘unknown’ of the <strong>housing</strong> and <strong>loan</strong> variables have a small number of observations and will be rolled into the second smallest category. Lastly, <strong>job</strong> and <strong>education</strong> would also benefit from grouping up of least common levels.</p>

<p>Moving on to the <strong>other campaign attributes</strong>:</p>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/other campaign attributes-1.png" alt="" width="100%" height="100%"/></p>

<p>Although continuous in principal, <strong>campaign</strong> is more categorical in nature and strongly right skewed, and will need to be discretised into groups. However, we have learned from the earlier correlation analysis that is not expected be a strong drivers of variation in any model.</p>

<p>On the other hand, <strong>poutcome</strong> is one of the attributes expected to be have a strong predictive power. The uneven distribution of levels would suggest to roll the least common occurring level (<strong>success</strong> or <code>scs</code>) into another category. However, contacting a client who previously purchased a term deposit is one of the catacteristics with highest predictive power and needs to be left ungrouped.</p>

<p>Then, I&rsquo;m looking at <strong>last contact information</strong>:</p>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/last contact information-1.png" alt="" width="100%" height="100%"/></p>

<p><strong>contact</strong> and <strong>month</strong> should impact the response variable as they both have a good level of variation compared to the target. <strong>month</strong> would also benefit from grouping up of least common levels.</p>

<p>In contrast, <strong>day_of_week</strong> does not appear to impact the response as there is not enough variation between the levels.</p>

<p>Last but not least, the <strong>social and economic attributes</strong>:</p>

<p><img src="/img/2020-02-14-propensity-modelling-1-of-3-data-preparation-and-exploratory-data-analysis/social-economic context attributes-1.png" alt="" width="100%" height="100%"/></p>

<p>All <em>social and economic attributes</em> show a good level of variation compared to the target variable, which suggests that they should all impact the response. They all display a <strong>high degree of multi-modality</strong> and do not have an even spread through the density plot, and will need to be binned.</p>

<p>It is also worth noting that, with the exception of _cons_confidence<em>index</em>, all other social and economic attributes are strongly correlated to each other, indicating that only one could be included in the model as they are all “picking up” similar economic trend.</p>

<h2 id="summary-of-exploratory-data-analysis-preparation">Summary of Exploratory Data Analysis &amp; Preparation</h2>

<ul>
<li><p>Correlation analysis with <strong>correlationfunnel</strong> helped identify a hierarchy of expected predictive power for all variables</p></li>

<li><p><strong>duration</strong> (eqpt_rent) has strongest correlation with target variable whereas some of the bank client data like <strong>housing</strong> and <strong>loan</strong> shows the weakest correlation</p></li>

<li><p>However, <strong>duration</strong> will <strong>NOT</strong> be used in the analysis as it is unknown before a call. As such it offers very little actionable insight or predictive value and should be discarded from any realistic predictive model</p></li>

<li><p>The target variable <strong>subscribed</strong> shows strong class imbalance, with nearly 89% of <strong>No churn</strong>, which will need to be addresses before the modelling analysis can begin</p></li>

<li><p>Most predictors would benefit from grouping up of least common levels</p></li>

<li><p>Further feature exploration revealed the most <strong>social and economic context attributes</strong>
are strongly correlated to each other, suggesting that only a selection of them could be considered in a final model</p></li>
</ul>

<h3 id="final-data-processing-and-transformation">Final Data Processing and Transformation</h3>

<p>Following up on the findings from the Exploratory Data Analysis, I&rsquo;ve discretised categorical and continuous predictors by combining least common levels into &ldquo;other&rsquo; category, set all variables but <code>age</code> as unordered factors ( <strong>h2o</strong> does not support ordered categorical variables) and shorted level names of some categorical variables to ease visualisations. You can find all the details and the full code on my webpage: <a href="https://diegousai.io/2020/02/propensity-modelling-1-of-3/"><strong>Propensity Modelling - Data Preparation and Exploratory Data Analysis</strong></a>.</p>

<p>Here I simply load up the final dataset hosted <a href="https://github.com/DiegoUsaiUK/Propensity_Modelling"> <strong>on my GitHub repo</strong></a>:</p>

<pre><code class="language-r">data_final &lt;- 
  readRDS(file = &quot;https://raw.githubusercontent.com/DiegoUsaiUK/Propensity_Modelling/master/01_data/data_final.rds&quot;)
</code></pre>

<h2 id="modelling-strategy">Modelling strategy</h2>

<p>In order to stick to a reasonable project running time, I&rsquo;ve opted for <strong>h2o</strong> as my modelling platform as it offers a number of advantages:</p>

<ul>
<li><p>it very easy to use and you can <strong>estimate several Machine Learning models</strong> in no time</p></li>

<li><p>it does <strong>not require to pre-treat character/factor variables by “binarising” them</strong> (this is done &ldquo;internally&rdquo;), which further reduces data formatting time</p></li>

<li><p>it has a functionality that <strong>takes care of the class imbalance</strong> highlighted in the Data Exploration phase - I simply set <code>balance_classes</code> = TRUE in the model specification, more on this later on</p></li>

<li><p><strong>cross-validation</strong> can be enabled without the need for a separate <code>validation frame</code> to be &ldquo;carved out&rdquo; of the training set</p></li>

<li><p><strong>hyper-parameters fine tuning</strong> (a.k.a. grid search) can be implemented alogside a number of strategies that ensure running time is capped without compromising on performance</p></li>
</ul>

<h3 id="building-models-with-h2o">Building models with h2o</h3>

<p>I&rsquo;m starting by creating a randomised training and validation set with <code>rsample</code> and save them as <code>train_tbl</code> and <code>test_tbl</code>.</p>

<pre><code class="language-r">set.seed(seed = 1975) 

train_test_split &lt;-
  rsample::initial_split(
    data = data_final,     
    prop = 0.80   
  ) 

train_tbl &lt;- train_test_split %&gt;% rsample::training() 
test_tbl  &lt;- train_test_split %&gt;% rsample::testing() 
</code></pre>

<p>Then, I start an <strong>h2o cluster</strong>. I specify the size of the memory cluster to “16G” to help speed things up a bit and turn off the progress bar.</p>

<pre><code class="language-r"># initialize h2o session and switch off progress bar
h2o.no_progress() 
h2o.init(max_mem_size = &quot;16G&quot;)

## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     C:\Users\LENOVO\AppData\Local\Temp\RtmpoJnEwn/h2o_LENOVO_started_from_r.out
##     C:\Users\LENOVO\AppData\Local\Temp\RtmpoJnEwn/h2o_LENOVO_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: ... Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         7 seconds 431 milliseconds 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.28.0.4 
##     H2O cluster version age:    2 months and 4 days  
##     H2O cluster name:           H2O_started_from_R_LENOVO_frm928 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   14.22 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4 
##     R Version:                  R version 3.6.3 (2020-02-29)
</code></pre>

<p>Next, I sort out response and predictor variables sets. For a classification to be performed, I need to <strong>ensure that the response variable is a factor</strong> (otherwise h2o will carry out a regression). This was sorted out during the data clensing and formatting phase.</p>

<pre><code class="language-r"># response variable
y &lt;- &quot;subscribed&quot;

# predictors set: remove response variable
x &lt;- setdiff(names(train_tbl %&gt;% as.h2o()), y)
</code></pre>

<h3 id="fitting-the-models">Fitting the models</h3>

<p>For this project I&rsquo;m estimating a <strong>Generalised Linear Model</strong> (a.k.a. Elastic Net), a <strong>Random Forest</strong> (which <strong>h2o</strong> refers to at <em>Distributed Random Forest</em>) and a <strong>Gradient Boosting Machine</strong> (or GBM).</p>

<p>To implement a grid search for the <code>tree-based</code> models (DRF and GBM), I need to set up a random grid to <strong>search for optimal hyper-parameters</strong> for the <code>h2o.grid()</code> function . To do so, I start with defining the <strong>search parameters</strong> to be passed to the <code>hyper_params</code>argument:</p>

<ul>
<li><p><code>sample_rate</code> is used to set the row sampling rate for each tree</p></li>

<li><p><code>col_sample_rate_per_tree</code> defines the column sampling for each tree</p></li>

<li><p><code>max_depth</code> specifies the maximum tree depth</p></li>

<li><p><code>min_rows</code> to fix the minimum number of observations per leaf</p></li>

<li><p><code>mtries</code>(DRF only) indicates the columns to randomly select on each node of the tree</p></li>

<li><p><code>learn_rate</code>(GBM only) specifies the rate at which the model learns when building a model</p>

<pre><code class="language-r"># DRF hyperparameters
hyper_params_drf &lt;- 
list(
 mtries                   = seq(2, 5, by = 1), 
 sample_rate              = c(0.65, 0.8, 0.95),
 col_sample_rate_per_tree = c(0.5, 0.9, 1.0),
 max_depth                = seq(1, 30, by = 3),
 min_rows                 = c(1, 2, 5, 10)
)

# GBM hyperparameters
hyper_params_gbm &lt;- 
list(
learn_rate               = c(0.01, 0.1),
sample_rate              = c(0.65, 0.8, 0.95),
col_sample_rate_per_tree = c(0.5, 0.9, 1.0),
max_depth                = seq(1, 30, by = 3),
min_rows                 = c(1, 2, 5, 10)
)
</code></pre></li>
</ul>

<p>I also set up a second list for the <code>search_criteria</code> argument, which helps to manage the models&rsquo; estimation running time:</p>

<ul>
<li><p>The <code>strategy</code> argument is set to <strong>RandomDiscrete</strong> for the search to randomly select a combination from the grid search parameters</p></li>

<li><p>Setting <code>stopping_metric</code> to AUC as the error metric for early stopping - the models will stop building new trees when the metric ceases to improve</p></li>

<li><p>With <code>stopping_rounds</code> I’m specifying the number of training rounds before early stopping is considered</p></li>

<li><p>I’m using <code>stopping_tolerance</code> to set minimal improvement needed for the training process to continue</p></li>

<li><p><code>max_runtime_secs</code> restricts the search time to <strong>one hour per model</strong></p>

<pre><code class="language-r">search_criteria_all &lt;- 
list(
  strategy           = &quot;RandomDiscrete&quot;,
  stopping_metric    = &quot;AUC&quot;,    
  stopping_rounds    = 10,
  stopping_tolerance = 0.0001,
  max_runtime_secs   = 60 * 60
)
</code></pre></li>
</ul>

<p>At last, I can set up the models&rsquo; formulations. Note that all models have 2 parameters in common:</p>

<ul>
<li><p>the <code>nfolds</code> parameter, which enables <strong>cross-validation</strong> to be carried out without the need for a validation_frame - if set to 5 for instance, it will perform a 5-fold cross-validation</p></li>

<li><p>the <code>balance_classes</code> parameter is set to <em>TRUE</em> to account for the imbalance in the target variable highlighted during the exploratory analysis. When enabled, h2o will either under-sample the majority class or oversample the minority class.</p>

<pre><code class="language-r"># elastic net model 
glm_model &lt;- 
h2o.glm(
x               = x,
y               = y, 
training_frame  = train_tbl %&gt;% as.h2o(),
balance_classes = TRUE,
nfolds          = 10,
family          = &quot;binomial&quot;,
seed            = 1975
)

# random forest model
drf_model_grid &lt;- 
h2o.grid(
algorithm       = &quot;randomForest&quot;, 
x               = x, 
y               = y,
training_frame  = train_tbl %&gt;% as.h2o(),
balance_classes = TRUE, 
nfolds          = 10,
ntrees          = 1000,
grid_id         = &quot;drf_grid&quot;,
hyper_params    = hyper_params_drf,
search_criteria = search_criteria_all,
seed            = 1975
)

# gradient boosting machine model
gbm_model_grid &lt;- 
h2o.grid(
algorithm       = &quot;gbm&quot;,
x               = x, 
y               = y,
training_frame  = train_tbl %&gt;% as.h2o(),
balance_classes = TRUE, 
nfolds          = 10,
ntrees          = 1000,
grid_id         = &quot;gbm_grid&quot;,
hyper_params    = hyper_params_gbm,
search_criteria = search_criteria_all,
seed            = 1975
)
</code></pre></li>
</ul>

<p>I sort the tree-based model by <em>AUC</em> score and retrieve the lead models from the grid</p>

<pre><code class="language-r"># Get the DRM grid results, sorted by AUC 
drf_grid_perf &lt;- 
  h2o.getGrid(grid_id     = &quot;drf_grid&quot;,
               sort_by    = &quot;AUC&quot;,
               decreasing = TRUE)

# Fetch the top DRF model, chosen by validation AUC
drf_model &lt;- 
  h2o.getModel(drf_grid_perf@model_ids[[1]])

# Get the GBM grid results, sorted by AUC 
gbm_grid_perf &lt;- 
  h2o.getGrid(grid_id     = &quot;gbm_grid&quot;,
               sort_by    = &quot;AUC&quot;,
               decreasing = TRUE)

# Fetch the top GBM model, chosen by validation AUC
gbm_model &lt;- 
  h2o.getModel(gbm_grid_perf@model_ids[[1]])
</code></pre>

<h2 id="performance-assessment">Performance assessment</h2>

<p>There are many libraries (like <em>IML</em>, <em>PDP</em>, <em>VIP</em>, and <em>DALEX</em> to name but the more popular) that help with <strong>Machine Learning Interpretability</strong>, <strong>feature explanation</strong> and <strong>general performance assessment</strong> and they all have gained in popularity in recent years.</p>

<p>There are a number of methodologies to interpret machine learning results (i.e. <em>local interpretable model-agnostic explanations</em>, <em>partial dependence plots</em>, <em>permutation-based variable importance</em>) but in this project I examine the <code>DALEX</code> package, which focuses on <strong>Model-Agnostic Interpretability</strong> and provides a convenient way to comparing performance across multiple models with different structures.</p>

<p>One of the key <strong>advantages</strong> of the model-agnostic approach used by <code>DALEX</code> is that you can <strong>compare contributions</strong> of traditional &ldquo;glass-box&rdquo; models to black-box models <strong>on the same scale</strong>. However, being permutation-based, one of its main <strong>drawbacks</strong> is that it does not scale well with large number of predictor variables and larger datasets.</p>

<h3 id="the-dalex-procedure">The DALEX procedure</h3>

<p>Currently <code>DALEX</code> does not support some of the more recent ML packages like <strong>h2o</strong> or <strong>xgboost</strong>. To make it compatible with such objects, I&rsquo;ve followed the procedure illustrated by <strong>Bradley Boehmke</strong> in his brilliant study <a href="https://uc-r.github.io/dalex"><strong>Model Interpretability with DALEX</strong></a>, from which I&rsquo;ve drawn lots of inspiration and borrowed some code.</p>

<p>First, the dataset needs to be in a specific format:</p>

<pre><code class="language-r"># convert feature variables to a data frame - tibble is also a data frame 
x_valid &lt;- test_tbl %&gt;% select(-subscribed) %&gt;% as_tibble()

# change response variable to a numeric binary vector
y_valid &lt;- as.vector(as.numeric(as.character(test_tbl$subscribed)))
</code></pre>

<p>Then, I create a predict function returning a vector of numeric values, which extracts the probability of the response for binary classification problems.</p>

<pre><code class="language-r"># create custom predict function
pred &lt;- function(model, newdata)  {
  results &lt;- as.data.frame(h2o.predict(model, newdata %&gt;% as.h2o()))
  return(results[[3L]])
  }
</code></pre>

<p>Now I can convert my machine learning models into DALEK &ldquo;explainers&rdquo; with the <code>explain()</code> function, which works as a &ldquo;container&rdquo; for the parameters.</p>

<pre><code class="language-r"># generalised linear model explainer
explainer_glm &lt;- explain(
  model            = glm_model, 
  type             = &quot;classification&quot;,
  data             = x_valid,
  y                = y_valid,
  predict_function = pred,
  label            = &quot;h2o_glm&quot;
  )

# random forest model explainer
explainer_drf &lt;- explain(
  model            = drf_model, 
  type             = &quot;classification&quot;,
  data             = x_valid,
  y                = y_valid,
  predict_function = pred,
  label            = &quot;h2o_drf&quot;
  )

# gradient boosting machine explainer
explainer_gbm &lt;- explain(
  model            = gbm_model, 
  type             = &quot;classification&quot;,
  data             = x_valid,
  y                = y_valid,
  predict_function = pred,
  label            = &quot;h2o_gbm&quot;
  )
</code></pre>

<h2 id="assessing-the-models">Assessing the models</h2>

<p>At last, I&rsquo;m ready to pass the <strong>explainer objects</strong> to several DALEX functions that will help assess and compare the performance of the different models. Given that performance measures may reflect a different aspect of the predictive performance of a model, it is <strong>important to evaluate and compare several metrics when appraising a model</strong> and DALEX does just that!</p>

<p>To evaluate and compare my models&rsquo; performance, I&rsquo;ve drawn inspiration from the framework used by <em>Przemyslaw Biecek</em> and <em>Tomasz Burzykowski</em> in their book, <a href="https://pbiecek.github.io/ema/introduction.html"><strong>Explanatory Model Analysis</strong></a>, which is structured around key questions:</p>

<ul>
<li><p>1 - Are the models well fitted?</p></li>

<li><p>2 - How do the models compare with one another?</p></li>

<li><p>3 - Which variables are important in the models?</p></li>

<li><p>4 - How does a single variable affect the average prediction?</p></li>
</ul>

<h3 id="1-are-the-models-well-fitted">1 - Are the models well fitted?</h3>

<h4 id="general-model-fit">General Model Fit</h4>

<p>To get an initial feel for how well my models fit the data, I can use the self-explanatory <code>model_performance()</code> function, which calculates selected model performance measures.</p>

<pre><code class="language-r">model_performance(explainer_glm)

## Measures for:  classification
## recall   : 0 
## precision: NaN 
## f1       : NaN 
## accuracy : 0.8914653 
## auc      : 0.7500738
## 
## Residuals:
##          0%         10%         20%         30%         40%         50% 
## -0.48867133 -0.16735197 -0.09713539 -0.07193152 -0.06273300 -0.05418778 
##         60%         70%         80%         90%        100% 
## -0.04661088 -0.03971492 -0.03265955  0.63246516  0.98072521
</code></pre>

<pre><code class="language-r">model_performance(explainer_drf)

## Measures for:  classification
## recall   : 0.1700224 
## precision: 0.76 
## f1       : 0.2778793 
## accuracy : 0.9040913 
## auc      : 0.7993824
## 
## Residuals:
##          0%         10%         20%         30%         40%         50% 
## -0.87841486 -0.13473277 -0.07933048 -0.06305297 -0.05556507 -0.04869549 
##         60%         70%         80%         90%        100% 
## -0.04172427 -0.03453394 -0.02891645  0.33089059  0.98046626
</code></pre>

<pre><code class="language-r">model_performance(explainer_gbm)

## Measures for:  classification
## recall   : 0.2192394 
## precision: 0.7340824 
## f1       : 0.33764 
## accuracy : 0.9066408 
## auc      : 0.7988382
## 
## Residuals:
##          0%         10%         20%         30%         40%         50% 
## -0.83600975 -0.14609749 -0.08115376 -0.06542395 -0.05572322 -0.04789869 
##         60%         70%         80%         90%        100% 
## -0.04068165 -0.03371074 -0.02750033  0.29004942  0.98274727
</code></pre>

<p>Based on the metrics available for all models ( <strong>accuracy</strong> and <strong>AUC</strong>), I can see that <strong>elastic net</strong> and <strong>gradient boosting</strong> are performing roughly on par with one another, with <strong>random forest</strong> not far behind. AUC ranges between .78-.80 whereas accuracy has a slightly narrower range of .89-.90</p>

<h4 id="residual-diagnostics">Residual diagnostics</h4>

<p>As shown in the previous paragraph, <code>model_performance()</code> also produces residual quantiles that can be plotted to compare absolute residual values across models.</p>

<pre><code class="language-r"># compute and assign residuals to an object
resids_glm &lt;- model_performance(explainer_glm)
resids_drf &lt;- model_performance(explainer_drf)
resids_gbm &lt;- model_performance(explainer_gbm)

# compare residuals plots
p1 &lt;- plot(resids_glm, resids_drf, resids_gbm) +
        theme_minimal() +
        theme(legend.position = 'bottom',
              plot.title = element_text(hjust = 0.5)) + 
        labs(y = '')
p2 &lt;- plot(resids_glm, resids_drf, resids_gbm, geom = &quot;boxplot&quot;) +
        theme_minimal() +
        theme(legend.position = 'bottom',
              plot.title = element_text(hjust = 0.5)) 

gridExtra::grid.arrange(p2, p1, nrow = 1)
</code></pre>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-16-1.png" alt="" width="100%" height="100%"/></p>

<p>The <strong>DRF and GBM</strong> models appear to perform <strong>on a par with one another</strong>, given the median absolute residuals. Looking at the residuals distribution on the right-hand side, you can see that the median residuals are the lowest for these two models, with <strong>the GLM seeing a higher number of tail residuals</strong>. This is also mirrored by the boxplots on the left-hand side, where the tree-based models both achieve the lowest median absolute residual value.</p>

<h3 id="2-how-do-the-models-compare-with-one-another">2 - How do the models compare with one another?</h3>

<h4 id="roc-and-auc">ROC and AUC</h4>

<p>The <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"><strong>Receiver Operating Characteristic (ROC)</strong></a> curve is a graphical method that allows to visualise a classification model performance against a random guess, which is represented by the striped line on the graph. The curve plots the true positive rate (TPR) on the y-axis against the false positive rate (FPR) on the x-axis.</p>

<pre><code class="language-r">eva_glm &lt;- DALEX::model_performance(explainer_glm)
eva_dfr &lt;- DALEX::model_performance(explainer_drf)
eva_gbm &lt;- DALEX::model_performance(explainer_gbm)

plot(eva_glm, eva_dfr, eva_gbm, geom = &quot;roc&quot;) +
  ggtitle(&quot;ROC Curves - All Models&quot;,  
          &quot;AUC_glm = 0.750  AUC_drf = 0.799  AUC_gbm = 0.798&quot;) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
</code></pre>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-17-1.png" alt="" width="100%" height="100%"/></p>

<p>The insight from a ROC curve is two-fold:</p>

<ul>
<li><p><strong>Direct read</strong>: All models are performing much better than a random guess</p></li>

<li><p><strong>Compared read</strong>: the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve"><strong>AUC (Area Under the Curve)</strong></a> summarises the ROC curve and can be used to directly compare models performance - the perfect classifier would have AUC = 1.</p></li>
</ul>

<p>All models performs much better that random guessing and achieves a AUC of .75-.80, with the DRF achieving the highest score of 0.799.</p>

<h3 id="3-which-variables-are-important-in-the-models">3 - Which variables are important in the models?</h3>

<h4 id="variable-importance-plots">Variable importance plots</h4>

<p>Each ML algorithm has its own way to assess the importance of each variable: linear models for instance refer to their coefficients, whereas tree-based models look at impurity, which makes it difficult to compare variable importance across models.</p>

<p>DALEX calculates variable importance measures via permutation, which is model agnostics and allows for <strong>direct comparison between models of different structure</strong>. However, when variable importance scores are based on permutations, we should remember that <strong>calculations slow down when the number of features increases</strong>.</p>

<p>Once again, I&rsquo;m passing the &ldquo;explainer&rdquo; for each single model to the <code>feature_importance()</code> function and setting <code>n_sample</code> to 8000 to use practically all available observations. Although not exorbitant, the total execution time was <strong>nearly 30 minute</strong> but this is based on a relatively small dataset and number of variables. Don&rsquo;t forget that computation speed can be increased by reducing <code>n_sample</code>, which is especially important for larger datasets.</p>

<pre><code class="language-r"># measure execution time
tictoc::tic()

# compute permutation-based variable importance
vip_glm &lt;- feature_importance(explainer_glm, n_sample = 8000,
                               loss_function = loss_root_mean_square) 

vip_drf &lt;- feature_importance(explainer_drf, n_sample = 8000, 
                               loss_function = loss_root_mean_square)

vip_gbm &lt;- feature_importance(explainer_gbm, n_sample = 8000, 
                               loss_function = loss_root_mean_square)

# show total execution time
tictoc::toc()

## 1663.78 sec elapsed
</code></pre>

<p>Now I only have to pass the <strong>vip</strong> objects to a plotting function: as suggested by the auto-generated x-axis label ( <strong>Drop-out loss</strong>), the main intuition behind how variable importance is calculated lies in how much the model fit would decrease if the contribution of a selected explanatory variable was removed. The larger the segment, the larger the loss when that variable is dropped from the model.</p>

<pre><code class="language-r"># plotting top 10 feature only for clarity of reading
plot(vip_glm, vip_drf, vip_gbm, max_vars = 10) +
  ggtitle(&quot;Permutation variable importance&quot;, 
          &quot;Average variable importance based on 8,000 permutations&quot;) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
</code></pre>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-19-1.png" alt="" width="100%" height="100%"/></p>

<p>I like this plot as it brings together a wealth of information.</p>

<p>First of all you can notice that, although with slightly different relative weights, the top 5 features are common to each models, with <code>nr_employed</code> ( <strong>employed in the economy</strong>) being the single most important predictor in all of them. This consistency is reassuring as it tells us that all models are picking up the same structure and interactions in the data, and gives us assurance that these features have strong predictive power.</p>

<p>You can also notice the <strong>distinct starting point</strong> for the x-axis left edge, which reflects the difference in the RMSE loss between the three models: in this case the <strong>elastic net</strong> model has the highest RMSE, suggesting the higher number of tail residuals seen earlier in the residual diagnostics is probably penalising the RMSE score.</p>

<h3 id="4-how-does-a-single-variable-affect-the-average-prediction">4 - How does a single variable affect the average prediction?</h3>

<h4 id="partial-dependence-profiles">Partial Dependence profiles</h4>

<p>After we have identified the relative predictive power of each variable, we may want to investigate how their relationship with the predicted response differ across all three models. <strong>Partial Dependence (PD) plots</strong>, sometimes also referred to as <em>PD profiles</em>, offer a great way to inspect how each model is responding to a particular predictor.</p>

<p>We can start with having a look at the single most important feature, <code>nr_employed</code>:</p>

<pre><code class="language-r"># compute PDP for a given variable
pdp_glm  &lt;- model_profile(explainer_glm, variable = &quot;nr_employed&quot;, type = &quot;partial&quot;)
pdp_drf  &lt;- model_profile(explainer_drf, variable = &quot;nr_employed&quot;, type = &quot;partial&quot;)
pdp_gbm  &lt;- model_profile(explainer_gbm, variable = &quot;nr_employed&quot;, type = &quot;partial&quot;)

plot(pdp_glm$agr_profiles, pdp_drf$agr_profiles, pdp_gbm$agr_profiles) +
  ggtitle(&quot;Contrastive Partial Dependence Profiles&quot;, &quot;&quot;) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
</code></pre>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-20-1.png" alt="" width="100%" height="100%"/></p>

<p>Although with different average prediction weights, all three models found that bank customers are more likely to sigh up to a term deposit when the level of <strong>employed in the economy</strong> is up to 5.099m (<code>nInf_5099.1</code>). Both <strong>elastic net</strong> and <strong>random forest</strong> have found the exact same hierarchy of predictive power among the 3 different levels of <code>nr_employed</code> (less pronounced for the <strong>random forest</strong>) that we observed in the <code>correlationfunnel</code> analysis, with <strong>GBM</strong> being the one slightly out of kilter.</p>

<p>Let&rsquo;s now take a look at <code>age</code>, a predictor that, if you recall from the EDA, was NOT expected to have an impact on the target variable:</p>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-21-1.png" alt="" width="100%" height="100%"/></p>

<p>One thing we notice is that the range of variation in the average prediction (x-axis) is relatively shallow across the age spectrum (y-axis), confirming the finding from the exploratory analysis that this variable would have a low predictive power. Also, both <strong>GBM</strong> and <strong>random forest</strong> are using <code>age</code> in a non-linear way, whereas the <strong>elastic net</strong> model is unable to capture this non-linear dynamic.</p>

<p>Partial Dependence plots could also work as a diagnostic tool: looking at <code>poutcome</code> (outcome of the <code>previous</code> marketing campaign) reveals that <strong>GBM</strong> and <strong>random forest</strong> correctly picked up on a higher probability of signing up when the outcome of a previous campaign was success (<code>scs</code>).</p>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-22-1.png" alt="" width="100%" height="100%"/></p>

<p>However, the <strong>elastic net</strong> model fails to do the same, which could represents a serious flaw as success in a previous campaign had a very strong positive correlation with the target variable.</p>

<p>I&rsquo;m going to finish with the <code>month</code> feature as it offers a great example of one of those cases where you may want to override the model&rsquo;s outcome with industry knowledge and some common sense. Specifically, the <strong>GBM</strong> model seems to suggest that <strong>March</strong>,  <strong>October</strong> and <strong>December</strong> are periods associated with much better odds of success.</p>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-23-1.png" alt="" width="100%" height="100%"/></p>

<p>Based on my previous analysis experience of similar financial products, I would not advise a banking organisation to ramp up their direct marketing activity around the weeks in the run to Christmas as this is a period of the year where the consumers&rsquo; focus shifts away from this type of purchases.</p>

<h2 id="final-model">Final model</h2>

<p>All in all <strong>random forest</strong> is my final model of choice: it appears the more balanced of the three and does not display some of the &ldquo;oddities&rdquo; seen with variables like <code>month</code> and <code>poutcome</code>.</p>

<p>I can now further refine my model and reduce its complexity by combining findings from the Exploratory analysis, insight from models&rsquo; assessment and a number of industry-specific/common sense considerations.</p>

<p>In particular, my <strong>final model</strong>:</p>

<ul>
<li><p>Excludes a number of features (<code>age</code>, <code>housing</code>, <code>loan</code>, <code>campaign</code>, <code>cons_price_idx</code>) that have low predictive power</p></li>

<li><p>Removes <code>previous</code>, which shows little difference between its 2 levels in the PD plot - it&rsquo;s also moderately correlated with <code>pdays</code>, suggesting that they may be capturing the same behaviour</p></li>

<li><p>Also drops <code>emp_var_rate</code> because of its strong correlation with <code>nr_employed</code> and also because conceptually they are controlling for a very similar economic behaviour</p>

<pre><code class="language-r"># response variable remains unaltered
y &lt;- &quot;subscribed&quot;

# predictors set: remove response variable and 7 predictors
x_final &lt;- setdiff(names(train_tbl %&gt;% 
                       select(-c(age, housing, loan, campaign, previous,
                                 cons_price_idx, emp_var_rate)) %&gt;% 
                       as.h2o()), y)
</code></pre></li>
</ul>

<p>For the final model, I&rsquo;m using the same specification as to the original random forest</p>

<pre><code class="language-r"># random forest model
drf_final &lt;- 
  h2o.grid(
     algorithm       = &quot;randomForest&quot;, 
     x               = x_final, 
     y               = y,
     training_frame  = train_tbl %&gt;% as.h2o(),
     balance_classes = TRUE, 
     nfolds          = 10,
     ntrees          = 1000,
     grid_id         = &quot;drf_grid_final&quot;,
     hyper_params    = hyper_params_drf,
     search_criteria = search_criteria_all,
     seed            = 1975
   )
</code></pre>

<p>Once again, we sort the model by AUC score and retrieve the lead model</p>

<pre><code class="language-r"># Get the grid results, sorted by AUC 
drf_grid_perf_final &lt;- 
  h2o.getGrid(grid_id = &quot;drf_grid_final&quot;,
               sort_by = &quot;AUC&quot;,
               decreasing = TRUE)

# Fetch the top DRF model, chosen by validation AUC
drf_final &lt;- 
  h2o.getModel(drf_grid_perf_final@model_ids[[1]])
</code></pre>

<h3 id="final-model-evaluation">Final model evaluation</h3>

<p>For brevity, I am visualising the variable importance plot with the <code>vip()</code> function from the namesake package, which returns the ranked contribution of each variable.</p>

<pre><code class="language-r">vip::vip(drf_final, num_features = 12) +
  ggtitle(&quot;Variable Importance&quot;, &quot;&quot;) + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
</code></pre>

<p><img src="/img/2020-03-17-propensity-modelling-2-of-3-estimate-and-compare-performance-of-several-models/unnamed-chunk-28-1.png" alt="" width="100%" height="100%"/></p>

<p>Removing <code>emp_var_rate</code> has allowed <code>education</code> to come into the top 10 features. Understandably, the variables hierarchy and relative predictive power has adjusted and changed slightly but it&rsquo;s reassuring to see that the other 9 variables were in the previous model&rsquo;s top 10.</p>

<p>Lastly, I&rsquo;m comparing the model’s performance with the original <strong>random forest</strong> model.</p>

<pre><code class="language-r">drf_final %&gt;% h2o.performance(newdata = test_tbl %&gt;% as.h2o()) %&gt;% h2o.auc()

## [1] 0.7926509
</code></pre>

<pre><code class="language-r">drf_model %&gt;% h2o.performance(newdata = test_tbl %&gt;% as.h2o()) %&gt;% h2o.auc()

## [1] 0.7993973
</code></pre>

<p>The <strong>AUC</strong> has only changed by a fraction of a percent, telling me that the model has maintained its predictive power.</p>

<h3 id="a-important-observation-on-partial-dependence-plots">A important observation on Partial Dependence Plots</h3>

<p>Being already familiar with odds ratios in the context of a logistic regression, I set out to understand whether the same intuition could be extended to black-box classification models. During my research one very interesting post on <a href="https://stats.stackexchange.com/"><strong>Cross Validated</strong></a> stood out for drawing a parallel between <a href="https://stats.stackexchange.com/questions/93202/odds-ratio-from-decision-tree-and-random-forest"><strong>odds ratio from decision tree and random forest</strong></a>.</p>

<p>Basically, this tells us that Partial Dependence plots can be used in a similar way to how odds ratios to define what characteristics of a customer profile influence his/her propensity to performing a certain type of behaviour.</p>

<p>For example, features like <code>job</code>, <code>month</code> and <code>contact</code> would provide context around <strong>who</strong>, <strong>when</strong> and <strong>how</strong> to target:</p>

<ul>
<li><p>Looking at <code>job</code> will tell us that a customer in an <em>admin</em> role is roughly 25% more likely to subscribe that a <em>self employed</em>.</p></li>

<li><p>Getting in touch with a prospective customer in the <code>month</code> of <em>October</em> will more then double the chance of a positive outcome than in <em>May</em>.</p></li>

<li><p><code>contacting</code> your customer on their <em>mobile</em> increases the chances of subscription by nearly a quarter compared to a <em>telephone</em> call.</p></li>
</ul>

<p><strong>NOTE THAT</strong> Partial Dependence Plots for all final model&rsquo;s predictors can be found on my webpage: on my webpage: <a href="https://diegousai.io/2020/03/propensity-modelling-2-of-3/"><strong>Propensity Modelling - Estimate Several Models and Compare Their Performance Using a Model-agnostic Methodology</strong></a>.</p>

<p>Armed with such insight, one can help <strong>shaping overall marketing and communication plans</strong> to focus on customers more likely to subscribe to a term deposit.</p>

<p>However, these are based on model-level explainers, which reflect an overall, aggregated view. If you&rsquo;re interested to understand how a model yields a prediction for a single observation (i.e. what factors influence the likelihood to engage <strong>at single customer level</strong>), you can resort to the <a href="https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/"><strong>Local Interpretable Model-agnostic Explanations (LIME)</strong></a> method that exploits the concept of a &ldquo;local model&rdquo;. I will be exploring the LIME methodology in a future post.</p>

<h2 id="summary-of-model-estimation-and-assessment">Summary of model estimation and assessment</h2>

<p>For the analysis part of this project I opted for <strong>h2o</strong> as my modelling platform. h2o is not only very easy to use but also has a number of built-in functionalities that help speeding up data preparation: it takes care of <strong>class imbalance</strong> with no need for pre-modelling resampling, automatically <strong>“binarises“ character/factor</strong> variables, and implements <strong>cross-validation</strong> without the need for a separate <code>validation frame</code> to be &ldquo;carved out&rdquo; of the training set.</p>

<p>After setting up a random grid to <strong>search for best hyper-parameters</strong>, I&rsquo;ve estimated a number of models ( a <em>logistic regression</em>, a <em>random forest</em> and a <em>gradient boosting machines</em>) and used the <strong>DALEX</strong> library to <strong>assess and compare their performance</strong> through an array of metrics. This library employs a <strong>model-agnostic approach</strong> that enables to <strong>compare traditional &ldquo;glass-box&rdquo; models and &ldquo;black-box&rdquo; models</strong> on the same scale.</p>

<p>My final model of choice is the <strong>random forest</strong>, which I further refined by combining findings from the exploratory analysis, insight gathered from the models&rsquo; evaluation and a number of industry-specific/common sense considerations. This ensured a reduced model complexity without compromising on predictive power.</p>

<h2 id="optimising-for-expected-profit">Optimising for expected profit</h2>

<p>Now that I have my final model, the last piece of the puzzle is the final “So what?” question that puts all into perspective. The estimate for the probability of a customer to sign up for a term deposit can be used to create a number of optimised scenarios, ranging from minimising your <strong>marketing expenditure</strong>, maximising your <strong>overall acquisition targets</strong>, to driving a certain number of <strong>cross-sell opportunities</strong>.</p>

<p>Before I can do that, there are a couple of <strong>housekeeping tasks</strong>  needed to &ldquo;set up the work scene&rdquo; and a couple of important concepts to introduce:</p>

<ul>
<li><p>the threshold and the F1 score</p></li>

<li><p>precision and recall</p></li>
</ul>

<h3 id="the-threshold-and-the-f1-score">The threshold and the F1 score</h3>

<p>The question the model is trying to answer is &ldquo; <em>Has this customer signed up for a term deposit following a direct marketing campaign?</em> &ldquo; and the cut-off (a.k.a. the threshold) is the value that divides the predictions into <code>Yes</code> and  <code>No</code>.</p>

<p>To illustrate the point, I first calculate some predictions by passing the <code>test_tbl</code> data set to the <code>h2o.performance</code> function.</p>

<pre><code class="language-r">perf_drf_final &lt;- h2o.performance(drf_final, newdata = test_tbl %&gt;% as.h2o()) 

perf_drf_final@metrics$max_criteria_and_metric_scores

## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold       value idx
## 1                       max f1  0.189521    0.508408 216
## 2                       max f2  0.108236    0.560213 263
## 3                 max f0point5  0.342855    0.507884 143
## 4                 max accuracy  0.483760    0.903848  87
## 5                max precision  0.770798    0.854167  22
## 6                   max recall  0.006315    1.000000 399
## 7              max specificity  0.930294    0.999864   0
## 8             max absolute_mcc  0.189521    0.444547 216
## 9   max min_per_class_accuracy  0.071639    0.721231 300
## 10 max mean_per_class_accuracy  0.108236    0.755047 263
## 11                     max tns  0.930294 7342.000000   0
## 12                     max fns  0.930294  894.000000   0
## 13                     max fps  0.006315 7343.000000 399
## 14                     max tps  0.006315  894.000000 399
## 15                     max tnr  0.930294    0.999864   0
## 16                     max fnr  0.930294    1.000000   0
## 17                     max fpr  0.006315    1.000000 399
## 18                     max tpr  0.006315    1.000000 399
</code></pre>

<p>Like many other machine learning modelling platforms, <strong>h2o</strong> uses the threshold value associated with the maximum <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a>, which is nothing but a weighted average between precision and recall. In this case the threshold @ Max F1 is <strong>0.190</strong>.</p>

<p>Now, I use the <code>h2o.predict</code> function to make predictions using the test set. The prediction output comes with three columns: the actual model predictions (<code>predict</code>), and the probabilities associated with that prediction (<code>p0</code>, and <code>p1</code>, corresponding to <code>No</code> and <code>Yes</code> respectively). As you can see, the <code>p1</code> probability associated with the current cut-off is around <strong>0.0646</strong>.</p>

<pre><code class="language-r">drf_predict &lt;- h2o.predict(drf_final, newdata = test_tbl %&gt;% as.h2o())

# I converte to a tibble for ease of use
as_tibble(drf_predict) %&gt;%
  arrange(p0) %&gt;% 
  slice(3088:3093) %&gt;%
  kable()
</code></pre>

<table>
<thead>
<tr>
<th align="left">predict</th>
<th align="left">p0</th>
<th align="left">p1</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">1</td>
<td align="left">0.9352865</td>
<td align="left">0.0647135</td>
</tr>

<tr>
<td align="left">1</td>
<td align="left">0.9352865</td>
<td align="left">0.0647135</td>
</tr>

<tr>
<td align="left">1</td>
<td align="left">0.9352865</td>
<td align="left">0.0647135</td>
</tr>

<tr>
<td align="left">0</td>
<td align="left">0.9354453</td>
<td align="left">0.0645547</td>
</tr>

<tr>
<td align="left">0</td>
<td align="left">0.9354453</td>
<td align="left">0.0645547</td>
</tr>

<tr>
<td align="left">0</td>
<td align="left">0.9354453</td>
<td align="left">0.0645547</td>
</tr>
</tbody>
</table>

<p>However, the <em>F1 score</em> is only one way to identify the cut-off. Depending on our goal, we could also decide to use a threshold that, for instance, maximises precision or recall. In a commercial setting, the pre-selected threshold @ Max F1 may not necessarily be the optimal choice: enter <strong>Precision and Recall</strong>!</p>

<h3 id="precision-and-recall">Precision and Recall</h3>

<p><strong>Precision</strong> shows how sensitive models are to False Positives (i.e. predicting a customer is <em>subscribing</em> when he-she is actually NOT) whereas <strong>Recall</strong> looks at how sensitive models are to False Negatives (i.e. forecasting that a customer is <em>NOT subscribing</em> whilst he-she is in fact going to do so).</p>

<p>These metrics are <strong>very relevant in a business context</strong> because organisations are particularly interested in accurately predicting which customers are truly likely to <code>subscribe</code> <strong>(high precision)</strong> so that they can target them with advertising strategies and other incentives. At the same time they want to minimise efforts towards customers incorrectly classified as <code>subscribing</code> <strong>(high recall)</strong> who are instead unlikely to sign up.</p>

<p>However, as you can see from the chart below, when precision gets higher, recall gets lower and vice versa. This is often referred to as the <strong>Precision-Recall tradeoff</strong>.</p>

<p><img src="/img/2020-04-20-propensity-modelling-3-of-3-optimise-profit-with-the-expected-value-framework/unnamed-chunk-6-1.png" alt="" width="100%" height="100%"/></p>

<p>To fully comprehend this dynamic and its implications, let&rsquo;s start with taking a look at the <strong>cut-off zero</strong> and <strong>cut-off one</strong> points and then see what happens when you start moving the threshold between the two positions:</p>

<ul>
<li><p>At <strong>threshold zero</strong> ( <em>lowest precision, highest recall</em>) the model classifies every customer as <code>subscribed = Yes</code>. In such scenario, you would <strong>contact every single customers</strong> with direct marketing activity but waste precious resourses by also including those less likely to subcsribe. Clearly this is not an optimal strategy as you&rsquo;d incur in a higher overall acquisition cost.</p></li>

<li><p>Conversely, at <strong>threshold one</strong> ( <em>highest precision, lowest recall</em>) the model tells you that nobody is likely to subscribe so you should <strong>contact no one</strong>. This would save you tons of money in marketing cost but you&rsquo;d be missing out on the additional revenue from those customers who would&rsquo;ve subscribed, had they been notified about the term deposit through direct marketing. Once again, not an optimal strategy.</p></li>
</ul>

<p>When moving to a higher threshold the model becomes more &ldquo;choosy&rdquo; on who it classifies as <code>subscribed = Yes</code>. As a consequence, you become more conservative on who to contact ( <strong>higher precision</strong>) and reduce your acquisition cost, but at the same time you increase your chance of not reaching prospective subscribes ( <strong>lower recall</strong>), missing out on potential revenue.</p>

<p>The key question here is <strong>where do you stop?</strong> Is there a &ldquo;sweet spot&rdquo; and if so, how do you find it? Well, that will depend entirely on the goal you want to achieve. In the next section I&rsquo;ll be running a mini-optimisation with the goal to <strong>maximise profit</strong>.</p>

<h2 id="finding-the-optimal-threshold">Finding the optimal threshold</h2>

<p>For this mini-optimisation I&rsquo;m implementing a <strong>simple profit maximisation</strong> based on generic costs connected to acquiring a new customer and benefits derived from said acquisition. This can be evolved to include more complex scenarios but it would be outside the scope of this exercise.</p>

<p>To understand which cut-off value is optimal to use we need to simulate the cost-benefit associated with each threshold point. This is a concept derived from the <strong>Expected Value Framework</strong> as seen on <a href="https://www.goodreads.com/book/show/17912916-data-science-for-business"><em>Data Science for Business</em></a></p>

<p>To do so I need 2 things:</p>

<ul>
<li><p><strong>Expected Rates for each threshold</strong> - These can be retrieved from the confusion matrix</p></li>

<li><p><strong>Cost/Benefit for each customer</strong> - I will need to simulate these based on assumptions</p></li>
</ul>

<p><strong>Expected rates</strong> can be conveniently retrieved for all cut-off points using <code>h2o.metric</code>.</p>

<pre><code class="language-r"># Get expected rates by cutoff
expected_rates &lt;- h2o.metric(perf_drf_final) %&gt;%
    as.tibble() %&gt;%
    select(threshold, tpr, fpr, fnr, tnr)
</code></pre>

<p>The <strong>cost-benefit matrix</strong> is a business assessment of the cost and benefit for each of four potential outcomes. To create such matrix I will have to make a few assumptions about the <strong>expenses and advantages</strong> that an organisation should consider when carrying out <strong>an advertising-led procurement drive</strong>.</p>

<p>Let&rsquo;s assume that the <strong>cost of selling a term deposits</strong> is of <strong>£30 per customer</strong>. This would include the likes of performing the direct marketing activity (training the call centre reps, setting time aside for active calls, etc.) and incentives such as offering a discounts on another financial product or on boarding onto membership schemes offering benefits and perks. A banking organisation will incur in this type of cost in two cases: when they correctly predict that a customer will subscribe ( <strong>true positive</strong>, TP), and when they incorrectly predict that a customer will subscribe ( <strong>false positive</strong>, FP).</p>

<p>Let’s also assume that the <strong>revenue of selling a term deposits</strong> to an existing customer is of <strong>£80 per customer</strong>. The organisation will guarantee this revenue stream when the model predicts that a customer will subscribe and they actually do ( <strong>true positive</strong>, TP).</p>

<p>Finally, there’s the <strong>true negative</strong> (TN) scenario where we correctly predict that a customer won’t subscribe. In this case we won’t spend any money but won&rsquo;t earn any revenue.</p>

<p>Here’s a quick recap of the scenarios:</p>

<ul>
<li><p><strong>True Positive</strong> (TP) - predict will subscribe, and they actually do: COST: -£30; REV £80</p></li>

<li><p><strong>False Positive</strong> (FP) - predict will subscribe, when they actually wouldn’t: COST: -£30; REV £0</p></li>

<li><p><strong>True Negative</strong> (TN) - predict won&rsquo;t subscribe, and they actually don’t: COST: £0; REV £0</p></li>

<li><p><strong>False Negative</strong> (FN) - predict won&rsquo;t subscribe, but they actually do: COST: £0; REV £0</p></li>
</ul>

<p>I create a function to calculate the expected profit using the probability of a <em>positive case</em> (p1) and the cost/benefit associated with a <em>true positive</em> (cb_tp) and a <em>false positive</em> (cb_fp). No need to include the <em>true negative</em> or <em>false negative</em> here as they&rsquo;re both zero.</p>

<p>I&rsquo;m also including the <strong>expected_rates</strong> data frame created previously with the expected rates for each threshold (400 thresholds, ranging from 0 to 1).</p>

<pre><code class="language-r"># Function to calculate expected profit
expected_profit_func &lt;- function(p1, cb_tp, cb_fp) {
  
    tibble(
        p1    = p1,
        cb_tp = cb_tp,
        cb_fp = cb_fp
        ) %&gt;%
    
        # add expected rates
        mutate(expected_rates = list(expected_rates)) %&gt;%
        unnest() %&gt;%
    
        # calculate the expected profit
        mutate(
            expected_profit =   p1    * (tpr * cb_tp) + 
                             (1 - p1) * (fpr * cb_fp)
        ) %&gt;%
        select(threshold, expected_profit)
}
</code></pre>

<h3 id="multi-customer-optimization">Multi-Customer Optimization</h3>

<p>Now to understand how a multi customer dynamic would work, I&rsquo;m creating a <strong>hypothetical 10 customer group</strong> to test my function on. This is a <strong>simplified</strong> view in that I&rsquo;m applying the <strong>same cost and revenue structure to all customers</strong> but the cost/benefit framework can be tailored to the individual customer to reflect their separate product and service level set up and the process can be easily adapted to optimise towards different KPIs (like <em>net profit</em>, <em>CLV</em>, <em>number of subscriptions</em>, etc.)</p>

<pre><code class="language-r"># Ten Hypothetical Customers 
ten_cust &lt;- tribble(
    ~&quot;cust&quot;,   ~&quot;p1&quot;,  ~&quot;cb_tp&quot;,  ~&quot;cb_fp&quot;,
    'ID1001',   0.1,    80 - 30,     -30,
    'ID1002',   0.2,    80 - 30,     -30,
    'ID1003',   0.3,    80 - 30,     -30,
    'ID1004',   0.4,    80 - 30,     -30,
    'ID1005',   0.5,    80 - 30,     -30,
    'ID1006',   0.6,    80 - 30,     -30,
    'ID1007',   0.7,    80 - 30,     -30,
    'ID1008',   0.8,    80 - 30,     -30,
    'ID1009',   0.9,    80 - 30,     -30,
    'ID1010',   1.0,    80 - 30,     -30
)
</code></pre>

<p>I use <code>purrr</code> to map the <code>expected_profit_func()</code> to each customer, returning a data frame of expected profit per customer by threshold value. This operation creates a nester tibble, which I have to <code>unnest()</code> to expand the data frame to one level.</p>

<pre><code class="language-r"># calculate expected profit for each at each threshold
expected_profit_ten_cust &lt;- ten_cust %&gt;%
    # pmap to map expected_profit_func() to each item
    mutate(expected_profit = pmap(.l = list(p1, cb_tp, cb_fp), 
                                  .f = expected_profit_func)) %&gt;%
    unnest() %&gt;%
    select(cust, p1, threshold, expected_profit) 
</code></pre>

<p>Then, I can visualize the expected profit curves for each customer.</p>

<pre><code class="language-r"># Visualising Expected Cost 
expected_profit_ten_cust %&gt;%
    ggplot(aes(threshold, expected_profit, 
               colour = factor(cust)), 
               group = cust) +
    geom_line(size = 1) +
    theme_minimal()  +
    tidyquant::scale_color_tq() +
    labs(title = &quot;Expected Profit Curves&quot;,
         colour = &quot;Customer No.&quot; ) +
    theme(plot.title = element_text(hjust = 0.5))
</code></pre>

<p><img src="/img/2020-04-20-propensity-modelling-3-of-3-optimise-profit-with-the-expected-value-framework/unnamed-chunk-11-1.png" alt="" width="100%" height="100%"/></p>

<p>Finally, I can aggregate the expected profit, visualise the final curve and highlight the optimal threshold.</p>

<pre><code class="language-r"># Aggregate expected profit by threshold 
total_expected_profit_ten_cust &lt;- expected_profit_ten_cust %&gt;%
    group_by(threshold) %&gt;%
    summarise(expected_profit_total = sum(expected_profit)) 

# Get maximum optimal threshold 
max_expected_profit &lt;- total_expected_profit_ten_cust %&gt;%
    filter(expected_profit_total == max(expected_profit_total))

# Visualize the total expected profit curve
total_expected_profit_ten_cust %&gt;%
    ggplot(aes(threshold, expected_profit_total)) +
    geom_line(size = 1) +
    geom_vline(xintercept = max_expected_profit$threshold) +
    theme_minimal() +
    labs(title = &quot;Expected Profit Curve - Total Expected Profit&quot;,
         caption  = paste0('threshold @ max = ', 
                          max_expected_profit$threshold %&gt;% round(3))) +
    theme(plot.title = element_text(hjust = 0.5))
</code></pre>

<p><img src="/img/2020-04-20-propensity-modelling-3-of-3-optimise-profit-with-the-expected-value-framework/unnamed-chunk-12-1.png" alt="" width="100%" height="100%"/></p>

<p>This has <strong>some important business implications</strong>. Based on our <em>hypothetical 10-customer group</em>, choosing the optimised threshold of <code>0.092</code> would yield a total profit of nearly <strong>£164</strong> compared to the nearly <strong>£147</strong> associated with the automatically selected cut-off of <code>0.190</code>.</p>

<p>This would result in an additional expected profit of <strong>nearly £1.7 per customer</strong>. Assuming that we have a customer base of approximately <strong>500,000</strong>, switching to the optimised model could generate an additional <strong>expected profit of £850k</strong>!</p>

<pre><code class="language-r">total_expected_profit_ten_cust %&gt;% 
  slice(184, 121) %&gt;%
  round(3) %&gt;%
  mutate(diff = expected_profit_total - lag(expected_profit_total)) 
## # A tibble: 2 x 3
##   threshold expected_profit_total  diff
##       &lt;dbl&gt;                 &lt;dbl&gt; &lt;dbl&gt;
## 1     0.19                   147.  NA  
## 2     0.092                  164.  16.9
</code></pre>

<p>It is easy to see that, depending on the size of your business, the magnitude of potential profit increase could be a significant.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>In this project, I&rsquo;ve used a publicly available dataset to estimate the likelihood of a bank&rsquo;s existing customers to purchase a financial product following a direct marketing campaign.</p>

<p>Following a thorough exploration and cleansing of the data, I estimate several models and compare their performance and fit to the data using the <strong>DALEX</strong> library, which focuses on <strong>Model-Agnostic Interpretability</strong>. One of its key <strong>advantages</strong> is the ability to compare contributions of traditional &ldquo;glass-box&rdquo; models as well as black-box models <strong>on the same scale</strong>. However, being permutation-based, one of its main <strong>drawbacks</strong> is that it does not scale well to large number of predictors and larger datasets.</p>

<p>Lastly, I take my <strong>final model</strong> and implemented a <strong>multi-customer profit optimization</strong> that reveals a potential additional expected profit of <strong>nearly £1.7 per customer</strong> (or <strong>£850k</strong> if you had a 500,000 customer base). Furthermore, I discuss key concepts like the <strong>threshold and F1 score</strong> and the <strong>precision-recall tradeoff</strong> and explain why it&rsquo;s highly important to decide which cutoff to adopt.</p>

<p>After exploring and cleansing the data, fitting and comparing multiple models and choosing the best one, sticking with the default threshold @ Max F1 would be stopping short of the ultimate &ldquo;so what?&rdquo; that puts all that hard work into prospective.</p>

<p><strong>One final thing</strong>: don’t forget to shut-down the h2o instance when you’re done!</p>

<pre><code class="language-r">h2o.shutdown(prompt = FALSE)
</code></pre>

<h3 id="code-repository">Code Repository</h3>

<p>The full R code and all relevant files can be found on my GitHub profile @ <a href="https://github.com/DiegoUsaiUK/Propensity_Modelling"><strong>Propensity Modelling</strong></a></p>

<h3 id="references">References</h3>

<ul>
<li><p>For the original paper that used the data set see: <a href="http://repositorium.sdum.uminho.pt/bitstream/1822/30994/1/dss-v3.pdf"><strong>A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems</strong></a>, S. Moro, P. Cortez and P. Rita.</p></li>

<li><p>To Speed Up <strong>Exploratory Data Analysis</strong> see: <a href="https://business-science.github.io/correlationfunnel/"><strong>correlationfunnel Package Vignette</strong></a></p></li>

<li><p>For a technically rigorous but applied take on <strong>Machine Learning Interpretability</strong> see Bradley Boehmke&rsquo;s <a href="https://uc-r.github.io/dalex"><strong>Model Interpretability with DALEX</strong></a></p></li>

<li><p>For a in-depth look at tools and techniques to <strong>examine fully-trained machine-learning models and compare their performance</strong> in a model-agnostic framework see: <a href="https://pbiecek.github.io/ema/introduction.html"><strong>Explanatory Model Analysis</strong></a>, P. Biecek, T. Burzykowski</p></li>

<li><p>For an advanced tutorial on sales forecasting and product backorders <strong>optimisation</strong> see Matt Dancho&rsquo;s <a href="https://www.business-science.io/business/2017/10/16/sales_backorder_prediction.html"><strong>Predictive Sales Analytics: Use Machine Learning to Predict and Optimize Product Backorders</strong></a></p></li>

<li><p>For the <strong>Expected Value Framework</strong> see: <a href="https://www.goodreads.com/book/show/17912916-data-science-for-business"><em>Data Science for Business</em></a></p></li>
</ul>
    
        </div>
    </section>

    <footer class="post-full-footer">
      <section class="author-card">
        <img class="author-profile-image" src="/img/Me_1.jpg" alt="Author" />
        <section class="author-card-content">
            <h4 class="author-card-name"><a href="/">Diego Usai</a></h4>
                <p></p>
        </section>
      </section>
    </footer>
</article>
    
    
    

  </div>
</main>


<aside class="read-next outer">
  <div class="inner">
    <div class="read-next-feed">      
      
<article class="read-next-card" 
            style="background-image: url(/img/simon-zhu-TfRHSL2GKDc-unsplash.jpg);" >
    <header class="read-next-card-header">
        <small class="read-next-card-header-sitetitle">&mdash; Lifelong Learning &mdash;</small>
        
        <h3 class="read-next-card-header-title"><a href="/tags/propensity-modelling/">#Propensity Modelling</a></h3>
    </header>
    <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
    </div>

    <div class="read-next-card-content">
      
        <ul>
          <li><a href="/2019/08/loading-merging-and-joining-datasets-postgresql-edt/">Loading, Merging and Several Joining Datasets - PostgreSQL EDT</a></li>            
        
          <li><a href="/2020/03/propensity-modelling-2-of-3/">Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Part 2 of 3 - Estimate Several Models and Compare Their Performance Using a Model-agnostic Methodology</a></li>            
        
          <li><a href="/2020/02/propensity-modelling-1-of-3/">Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Part 1 of 3 - Data Preparation and Exploratory Data Analysis</a></li>            
        
                      
        
          <li><a href="/2019/03/market-basket-analysis-part-2-of-3/">Market Basket Analysis - Part 2 of 3 - Market Basket Analysis with recommenderlab</a></li>            
        </ul>
    </div>
    <footer class="read-next-card-footer">
      
        <a href="/tags/propensity-modelling/">See all related posts →</a>
    </footer>
</article>


      
      
      <article class="post-card post"> 
    
    <a class="post-card-image-link" href="/2020/04/propensity-modelling-3-of-3/">
      <div class="post-card-image" style="background-image: url(/img/david-sury-X6M3svSX8dc-unsplash.jpg)"></div>
    </a>
    

    <div class="post-card-content">
      <a class="post-card-content-link" href="/2020/04/propensity-modelling-3-of-3/">
          <header class="post-card-header">
              <span class="post-card-tags">
              #Propensity Modelling 
              #Machine Learning 
              #Optimisation  </span>
              
              <h2 class="post-card-title">Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Part 3 of 3 - Optimise Profit With the Expected Value Framework</h2>
          </header>
          <section class="post-card-excerpt">
              
                <p>In this day and age, a business that leverages data to understand the drivers of its customers&rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease out this type of insight is Propensity Modelling, which combines information such as a customers’ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc. ...  </p>
              
          </section>
      </a>

      <footer class="post-card-meta">
          <img class="author-profile-image" src="/img/Me_1.jpg" alt="Author" />
          <span class="post-card-author"><a href="/">Diego Usai</a></span>
      </footer>
    </div>
</article>
      
    </div>
  </div>
</aside>

<div class="floating-header">
  <div class="floating-header-logo">
    <a href="/">
      
      <span></span>
    </a>
  </div>
  <span class="floating-header-divider">&mdash;</span>
  <div class="floating-header-title">Propensity Modelling - Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product - Abridged Version</div>
  <div class="floating-header-share">
    <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
     <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/></svg>
    </div>
    
    <a class="floating-header-share-tw" href="https://twitter.com/share?text=Propensity%20Modelling%20-%20Using%20h2o%20and%20DALEX%20to%20Estimate%20Likelihood%20to%20Purchase%20a%20Financial%20Product%20-%20Abridged%20Version&amp;url=%2f2020%2f05%2fpropensity-modelling-abridged%2f"
          onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
      </a>
      <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=%2f2020%2f05%2fpropensity-modelling-abridged%2f"
          onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
      </a>
  </div>

  <progress class="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</div>



<footer class="site-footer outer">
  <div class="site-footer-content inner">
    <section class="copyright" style="line-height: 1.3em;">
      <a href="/">Diego Usai</a> © 2019 <br>
      <span style="font-size: 0.8em; color: #555;">Hugo port of <a href="https://github.com/TryGhost/Casper">Casper 2.1.7</a> by <a href="https://www.telematika.org">EM</a></span>
    </section>
    <nav class="site-footer-nav">
        <a href="/">Latest Posts</a>
        
        
        <a href="https://github.com/DiegoUsaiUK" target="_blank" rel="noopener">Github</a>
        <a href="https://www.linkedin.com/in/diegousaiuk" target="_blank" rel="noopener">LinkedIn</a>
        <a href="https://medium.com/@diegousaiuk" target="_blank" rel="noopener">Medium</a>
    </nav>  
  </div>
</footer>

</div>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script type="text/javascript" src="//code.jquery.com/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="/js/jquery.fitvids.js"></script>

<script>hljs.initHighlightingOnLoad();</script>


  <!-- Google Analytics -->
  <script>
    var _gaq=[['_setAccount','UA-151122416-1'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
  </script>


    <script>





$(document).ready(function () {
    
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>
</body></html>
