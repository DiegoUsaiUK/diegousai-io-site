<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>/post/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-UK</language>
    <lastBuildDate>Sat, 02 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Propensity Modelling - Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product - Abridged Version</title>
      <link>/2020/05/propensity-modelling-abridged/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/propensity-modelling-abridged/</guid>
      <description>In this day and age, a business that leverages data to understand the drivers of customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease this type of insight out of data is Propensity Modelling, which combines information such as a customers’ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.</description>
    </item>
    
    <item>
      <title>Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Part 3 of 3 - Optimise Profit With the Expected Value Framework</title>
      <link>/2020/04/propensity-modelling-3-of-3/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/propensity-modelling-3-of-3/</guid>
      <description>In this day and age, a business that leverages data to understand the drivers of its customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease out this type of insight is Propensity Modelling, which combines information such as a customers’ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.</description>
    </item>
    
    <item>
      <title>Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Part 2 of 3 - Estimate Several Models and Compare Their Performance Using a Model-agnostic Methodology</title>
      <link>/2020/03/propensity-modelling-2-of-3/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/propensity-modelling-2-of-3/</guid>
      <description>In this day and age, a business that leverages data to understand the drivers of its customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease out this type of insight is Propensity Modelling, which combines information such as a customers’ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.</description>
    </item>
    
    <item>
      <title>Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Part 1 of 3 - Data Preparation and Exploratory Data Analysis</title>
      <link>/2020/02/propensity-modelling-1-of-3/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/propensity-modelling-1-of-3/</guid>
      <description>In this day and age, a business that leverages data to understand the drivers of its customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease out this type of insight is Propensity Modelling, which combines information such as a customers’ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.</description>
    </item>
    
    <item>
      <title>Time Series Machine Learning Analysis and Demand Forecasting with H2O &amp; TSstudio</title>
      <link>/2019/12/time-series-machine-learning-analysis-and-demand-forecasting/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/time-series-machine-learning-analysis-and-demand-forecasting/</guid>
      <description>Traditional approaches to time series analysis and forecasting, like Linear Regression, Holt-Winters Exponential Smoothing, ARMA/ARIMA/SARIMA and ARCH/GARCH, have been well-established for decades and find applications in fields as varied as business and finance (e.g. predict stock prices and analyse trends in financial markets), the energy sector (e.g. forecast electricity consumption) and academia (e.g. measure socio-political phenomena).
In more recent times, the popularisation and wider availability of open source frameworks like Keras, TensorFlow and scikit-learn helped machine learning approaches like Random Forest, Extreme Gradient Boosting, Time Delay Neural Network and Recurrent Neural Network to gain momentum in time series applications.</description>
    </item>
    
    <item>
      <title>Build Your Website with Hugo and blogdown - How I used RStudio, GitHub and Netlify to create and deploy my own webpage</title>
      <link>/2019/10/build-your-website-with-hugo-and-blogdown/</link>
      <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/build-your-website-with-hugo-and-blogdown/</guid>
      <description>This year has been rather rewarding for me! After completing some of the excellent Business Science University courses, I have worked on a number of Customer Analytics &amp;amp; Business Intelligence projects and summarised them into technical articles that I published on Medium&amp;rsquo;s Towards Data Science. This opened up an entirely new world to me and generated many new connections within the analytics and data science community the world over!</description>
    </item>
    
    <item>
      <title>Steps and considerations to run a successful segmentation with K-means, Principal Components Analysis and Bootstrap Evaluation</title>
      <link>/2019/09/steps-and-considerations-to-run-a-successful-segmentation/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/steps-and-considerations-to-run-a-successful-segmentation/</guid>
      <description>Clustering is one of my favourite analytic methods: it resonates well with clients as I&amp;rsquo;ve found from my consulting experience, and is a relatively straightforward concept to explain non technical audiences.
Earlier this year I&amp;rsquo;ve used the popular K-Means clustering algorithm to segment customers based on their response to a series of marketing campaigns. For that post I&amp;rsquo;ve deliberately used a basic dataset to show that it is not only a relatively easy analysis to carry out but can also help unearthing interesting patterns of behaviour in your customer base even when using few customer attributes.</description>
    </item>
    
    <item>
      <title>Loading, Merging and Several Joining Datasets - PostgreSQL EDT</title>
      <link>/2019/08/loading-merging-and-joining-datasets-postgresql-edt/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/loading-merging-and-joining-datasets-postgresql-edt/</guid>
      <description>This is the coding necessary to assemble the various data feeds and sort out the likes of variables naming &amp;amp; new features creation plus some general housekeeping tasks.
In order to simulate normal working conditions I would face if the data was stored on a database, I&amp;rsquo;ve uploaded the excel files onto a local PostgreSQL database that I&amp;rsquo;ve created on my machine. I am going to go through the steps I followed to set up a connection between RStudio and said database and extract the information I needed.</description>
    </item>
    
    <item>
      <title>Loading, Merging and Several Joining Datasets - Excel EDT</title>
      <link>/2019/07/loading-merging-and-joining-datasets-excel-edt/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/loading-merging-and-joining-datasets-excel-edt/</guid>
      <description>This is the minimal coding necessary to assemble various data feeds and sort out the likes of variables naming &amp;amp; new features creation plus some general housekeeping tasks. It includes general housekeeping tasks like shortening variables names to ease visualisations, creating essential new features and sorting out variables order
The Dataset library(tidyverse) library(lubridate) library(readr)  The dataset I&amp;rsquo;m using here accompanies a Redbooks publication called Building 360-Degree Information Applications which is available as a free PDF download.</description>
    </item>
    
    <item>
      <title>Modelling with Tidymodels and Parsnip - A Tidy Approach to a Classification Problem</title>
      <link>/2019/06/modelling-with-tidymodels-and-parsnip/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/modelling-with-tidymodels-and-parsnip/</guid>
      <description>Recently I have completed the online course Business Analysis With R focused on applied data and business science with R, which introduced me to a couple of new modelling concepts and approaches. One that especially captured my attention is parsnip and its attempt to implement a unified modelling and analysis interface (similar to python&amp;rsquo;s scikit-learn) to seamlessly access several modelling platforms in R.
parsnip is the brainchild of RStudio&amp;rsquo;s Max Khun (of caret fame) and Davis Vaughan and forms part of tidymodels, a growing ensemble of tools to explore and iterate modelling tasks that shares a common philosophy (and a few libraries) with the tidyverse.</description>
    </item>
    
    <item>
      <title>A gentle Introduction to Customer Segmentation - Using K-Means Clustering to Understand Marketing Response</title>
      <link>/2019/05/a-gentle-introduction-to-customer-segmentation/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/a-gentle-introduction-to-customer-segmentation/</guid>
      <description>Market segmentation refers to the process of dividing a consumer market of existing and/or potential customers into groups (or segments) based on shared attributes, interests, and behaviours.
For this mini-project I will use the popular K-Means clustering algorithm to segment customers based on their response to a series of marketing campaigns. The basic concept is that consumers who share common traits would respond to marketing communication in a similar way so that companies can reach out for each group in a relevant and effective way.</description>
    </item>
    
    <item>
      <title>Market Basket Analysis - Part 3 of 3 - A Shiny Product Recommender with Improved Collaborative Filtering</title>
      <link>/2019/04/market-basket-analysis-part-3-of-3/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/market-basket-analysis-part-3-of-3/</guid>
      <description>My objective for this piece of work is to carry out a Market Basket Analysis as an end-to-end data science project. I have split the output into three parts, of which this is the THIRD and last, that I have organised as follows:
 In the first chapter, I will source, explore and format a complex dataset suitable for modelling with recommendation algorithms.
 For the second part, I will apply various machine learning algorithms for Product Recommendation and select the best performing model.</description>
    </item>
    
    <item>
      <title>Market Basket Analysis - Part 2 of 3 - Market Basket Analysis with recommenderlab</title>
      <link>/2019/03/market-basket-analysis-part-2-of-3/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/market-basket-analysis-part-2-of-3/</guid>
      <description>My objective for this piece of work is to carry out a Market Basket Analysis as an end-to-end data science project. I have split the output into three parts, of which this is the SECOND, that I have organised as follows:
 In the first chapter, I will source, explore and format a complex dataset suitable for modelling with recommendation algorithms.
 For the second part, I will apply various machine learning algorithms for Product Recommendation and select the best performing model.</description>
    </item>
    
    <item>
      <title>Market Basket Analysis - Part 1 of 3 - Data Preparation and Exploratory Data Analysis</title>
      <link>/2019/03/market-basket-analysis-part-1-of-3/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/market-basket-analysis-part-1-of-3/</guid>
      <description>My objective for this piece of work is to carry out a Market Basket Analysis as an end-to-end data science project. I have split the output into three parts, of which this is the FIRST, that I have organised as follows:
 In the first chapter, I will source, explore and format a complex dataset suitable for modelling with recommendation algorithms.
 For the second part, I will apply various machine learning algorithms for Product Recommendation and select the best performing model.</description>
    </item>
    
  </channel>
</rss>