<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Exploration on </title>
    <link>/tags/data-exploration/</link>
    <description>Recent content in Data Exploration on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-UK</language>
    <lastBuildDate>Sat, 02 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/data-exploration/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Propensity Modelling - Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product - Abridged Version</title>
      <link>/2020/05/propensity-modelling-abridged/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/propensity-modelling-abridged/</guid>
      <description>In this day and age, a business that leverages data to understand the drivers of customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease this type of insight out of data is Propensity Modelling, which combines information such as a customers’ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.</description>
    </item>
    
    <item>
      <title>Segmenting with Mixed Type Data - Initial data inspection and manupulation</title>
      <link>/2020/04/mixed-type-data-segmentation-initial-data-exploration/</link>
      <pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/mixed-type-data-segmentation-initial-data-exploration/</guid>
      <description>With the new year, I started to look for new employment opportunities and even managed to land a handful of final stage interviews before it all grounded to a halt following the coronavirus pandemic. Invariably, as part of the selection process I was asked to analyse a set of data and compile a number of data driven-recommendations to present in my final meeting.
In this post I retrace the steps I took for one of the take home analysis I was tasked with and revisit clustering, one of my favourite analytic methods.</description>
    </item>
    
    <item>
      <title>Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Data Preparation and Exploratory Data Analysis</title>
      <link>/2020/01/propensity-modelling-data-preparation/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/01/propensity-modelling-data-preparation/</guid>
      <description>In this day and age, a business that leverages data to understand the drivers of its customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease out this type of insight is Propensity Modelling, which combines information such as a customers’ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.</description>
    </item>
    
    <item>
      <title>Time Series Machine Learning Analysis and Demand Forecasting with H2O &amp; TSstudio</title>
      <link>/2019/12/time-series-machine-learning-analysis-and-demand-forecasting/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/time-series-machine-learning-analysis-and-demand-forecasting/</guid>
      <description>Traditional approaches to time series analysis and forecasting, like Linear Regression, Holt-Winters Exponential Smoothing, ARMA/ARIMA/SARIMA and ARCH/GARCH, have been well-established for decades and find applications in fields as varied as business and finance (e.g. predict stock prices and analyse trends in financial markets), the energy sector (e.g. forecast electricity consumption) and academia (e.g. measure socio-political phenomena).
In more recent times, the popularisation and wider availability of open source frameworks like Keras, TensorFlow and scikit-learn helped machine learning approaches like Random Forest, Extreme Gradient Boosting, Time Delay Neural Network and Recurrent Neural Network to gain momentum in time series applications.</description>
    </item>
    
    <item>
      <title>Loading, Merging and Joining Several Datasets - PostgreSQL EDT</title>
      <link>/2019/08/loading-merging-and-joining-datasets-postgresql-edt/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/loading-merging-and-joining-datasets-postgresql-edt/</guid>
      <description>This is the coding necessary to assemble the various data feeds and sort out the likes of variables naming &amp;amp; new features creation plus some general housekeeping tasks.
In order to simulate normal working conditions I would face if the data was stored on a database, I&amp;rsquo;ve uploaded the excel files onto a local PostgreSQL database that I&amp;rsquo;ve created on my machine. I am going to go through the steps I followed to set up a connection between RStudio and said database and extract the information I needed.</description>
    </item>
    
    <item>
      <title>Loading, Merging and Joining Several Datasets - Excel EDT</title>
      <link>/2019/07/loading-merging-and-joining-datasets-excel-edt/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/loading-merging-and-joining-datasets-excel-edt/</guid>
      <description>This is the minimal coding necessary to assemble various data feeds and sort out the likes of variables naming &amp;amp; new features creation plus some general housekeeping tasks. It includes general housekeeping tasks like shortening variables names to ease visualisations, creating essential new features and sorting out variables order
The Dataset library(tidyverse) library(lubridate) library(readr)  The dataset I&amp;rsquo;m using here accompanies a Redbooks publication called Building 360-Degree Information Applications which is available as a free PDF download.</description>
    </item>
    
    <item>
      <title>A gentle Introduction to Customer Segmentation - Using K-Means Clustering to Understand Marketing Response</title>
      <link>/2019/05/a-gentle-introduction-to-customer-segmentation/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/a-gentle-introduction-to-customer-segmentation/</guid>
      <description>Market segmentation refers to the process of dividing a consumer market of existing and/or potential customers into groups (or segments) based on shared attributes, interests, and behaviours.
For this mini-project I will use the popular K-Means clustering algorithm to segment customers based on their response to a series of marketing campaigns. The basic concept is that consumers who share common traits would respond to marketing communication in a similar way so that companies can reach out for each group in a relevant and effective way.</description>
    </item>
    
    <item>
      <title>Market Basket Analysis - Part 1 of 3 - Data Preparation and Exploratory Data Analysis</title>
      <link>/2019/03/market-basket-analysis-part-1-of-3/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/market-basket-analysis-part-1-of-3/</guid>
      <description>My objective for this piece of work is to carry out a Market Basket Analysis as an end-to-end data science project. I have split the output into three parts, of which this is the FIRST, that I have organised as follows:
 In the first chapter, I will source, explore and format a complex dataset suitable for modelling with recommendation algorithms.
 For the second part, I will apply various machine learning algorithms for Product Recommendation and select the best performing model.</description>
    </item>
    
  </channel>
</rss>