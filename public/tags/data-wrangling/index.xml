<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Wrangling on </title>
    <link>/tags/data-wrangling/</link>
    <description>Recent content in Data Wrangling on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-UK</language>
    <lastBuildDate>Fri, 14 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/data-wrangling/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Propensity Modelling - Using h2o and DALEX to Estimate the Likelihood of Purchasing a Financial Product - Part 1 of 3 - Data Preparation and Exploratory Data Analysis</title>
      <link>/2020/02/propensity-modelling-1-of-3/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/propensity-modelling-1-of-3/</guid>
      <description>In this day and age, a business that leverages data to understand the drivers of its customers&amp;rsquo; behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage.
One trialled and tested approach to tease out this type of insight is Propensity Modelling, which combines information such as a customersâ€™ demographics (age, race, religion, gender, family size, ethnicity, income, education level), psycho-graphic (social class, lifestyle and personality characteristics), engagement (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.</description>
    </item>
    
    <item>
      <title>Loading, Merging and Several Joining Datasets - PostgreSQL EDT</title>
      <link>/2019/08/loading-merging-and-joining-datasets-postgresql-edt/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/loading-merging-and-joining-datasets-postgresql-edt/</guid>
      <description>This is the coding necessary to assemble the various data feeds and sort out the likes of variables naming &amp;amp; new features creation plus some general housekeeping tasks.
In order to simulate normal working conditions I would face if the data was stored on a database, I&amp;rsquo;ve uploaded the excel files onto a local PostgreSQL database that I&amp;rsquo;ve created on my machine. I am going to go through the steps I followed to set up a connection between RStudio and said database and extract the information I needed.</description>
    </item>
    
    <item>
      <title>Loading, Merging and Several Joining Datasets - Excel EDT</title>
      <link>/2019/07/loading-merging-and-joining-datasets-excel-edt/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/loading-merging-and-joining-datasets-excel-edt/</guid>
      <description>This is the minimal coding necessary to assemble various data feeds and sort out the likes of variables naming &amp;amp; new features creation plus some general housekeeping tasks. It includes general housekeeping tasks like shortening variables names to ease visualisations, creating essential new features and sorting out variables order
The Dataset library(tidyverse) library(lubridate) library(readr)  The dataset I&amp;rsquo;m using here accompanies a Redbooks publication called Building 360-Degree Information Applications which is available as a free PDF download.</description>
    </item>
    
    <item>
      <title>Market Basket Analysis - Part 1 of 3 - Data Preparation and Exploratory Data Analysis</title>
      <link>/2019/03/market-basket-analysis-part-1-of-3/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/market-basket-analysis-part-1-of-3/</guid>
      <description>My objective for this piece of work is to carry out a Market Basket Analysis as an end-to-end data science project. I have split the output into three parts, of which this is the FIRST, that I have organised as follows:
 In the first chapter, I will source, explore and format a complex dataset suitable for modelling with recommendation algorithms.
 For the second part, I will apply various machine learning algorithms for Product Recommendation and select the best performing model.</description>
    </item>
    
  </channel>
</rss>